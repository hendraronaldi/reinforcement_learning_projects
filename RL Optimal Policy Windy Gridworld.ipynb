{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RL Optimal Policy Windy Gridworld.ipynb","provenance":[{"file_id":"1igBjewh88JO188oWAuZhjBliHwFvAXhD","timestamp":1597989972310},{"file_id":"1XqgnGN7Cc9LSq3jFXgAo2FPFUT9_oFuY","timestamp":1597986608417}],"collapsed_sections":["6deyZkgz62ff"],"authorship_tag":"ABX9TyNI3lFuCm3jMYTMNmqp/Qpo"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"wqk9u4Y6c1ju","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598005020660,"user_tz":-420,"elapsed":1996,"user":{"displayName":"Hendra Ronaldi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1GApVeWOLfLtzaKM_vycD2e04Atp2_i5j6_Ol=s64","userId":"08624543525853224756"}}},"source":["import numpy as np"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"id":"ae5y3AdmK4eB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598005023288,"user_tz":-420,"elapsed":4613,"user":{"displayName":"Hendra Ronaldi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1GApVeWOLfLtzaKM_vycD2e04Atp2_i5j6_Ol=s64","userId":"08624543525853224756"}}},"source":["ACTION_SPACE = ('U', 'D', 'L', 'R')"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"id":"4y21_Xvpc-MI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598005023289,"user_tz":-420,"elapsed":4610,"user":{"displayName":"Hendra Ronaldi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1GApVeWOLfLtzaKM_vycD2e04Atp2_i5j6_Ol=s64","userId":"08624543525853224756"}}},"source":["class WindyGrid:\n","    def __init__(self, rows, columns, start):\n","        self.rows = rows\n","        self.columns = columns\n","        self.i = start[0]\n","        self.j = start[1]\n","\n","    def set_rewards_actions_probs(self, rewards, actions, probs):\n","        self.rewards = rewards # {s: r}\n","        self.actions = actions # {s: (a)}\n","        self.probs = probs # transition probabilities -> nested dict {(s,a): {(s'): prob}}\n","    \n","    def get_state(self):\n","        return (self.i, self.j)\n","\n","    def set_state(self, s):\n","        self.i = s[0]\n","        self.j = s[1]\n","\n","    # def get_next_state(self, s, a) is removed # -> because now next state is probabilistic so we can't know for sure the next state \n","    \n","    def move(self, a):\n","        next_state_probs = self.probs[((self.i, self.j), a)]\n","        next_states = next_state_probs.keys()\n","        next_probs = next_state_probs.values()\n","\n","        next_state = np.random.choice(next_states, p=next_probs) # random choice for each next states with their corresponding probability\n","\n","        self.i, self.j = next_state\n","\n","        return self.rewards.get(next_state, 0)\n","    \n","    def is_terminal(self, s):\n","        return s not in self.actions\n","\n","    def game_over(self):\n","        return (self.i, self.j) not in self.actions\n","\n","    def all_states(self):\n","        return set(self.actions.keys() | self.rewards.keys())"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsLDOoqVdlmM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598005023293,"user_tz":-420,"elapsed":4605,"user":{"displayName":"Hendra Ronaldi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1GApVeWOLfLtzaKM_vycD2e04Atp2_i5j6_Ol=s64","userId":"08624543525853224756"}}},"source":["def grid_windy():\n","    g = WindyGrid(3, 4, (2, 0))\n","    rewards = {\n","        (0,3): 1,\n","        (1,3): -1\n","    }\n","    \n","    actions = {\n","        (0, 0): ('D', 'R'),\n","        (0, 1): ('L', 'R'),\n","        (0, 2): ('L', 'D', 'R'),\n","        (1, 0): ('U', 'D'),\n","        (1, 2): ('U', 'D', 'R'),\n","        (2, 0): ('U', 'R'),\n","        (2, 1): ('L', 'R'),\n","        (2, 2): ('U', 'L', 'R'),\n","        (2, 3): ('U', 'L')\n","    }\n","\n","    probs = {\n","        ((2, 0), 'U'): {(1, 0): 1.0},\n","        ((2, 0), 'D'): {(2, 0): 1.0},\n","        ((2, 0), 'L'): {(2, 0): 1.0},\n","        ((2, 0), 'R'): {(2, 1): 1.0},\n","        ((1, 0), 'U'): {(0, 0): 1.0},\n","        ((1, 0), 'D'): {(2, 0): 1.0},\n","        ((1, 0), 'L'): {(1, 0): 1.0},\n","        ((1, 0), 'R'): {(1, 0): 1.0},\n","        ((0, 0), 'U'): {(0, 0): 1.0},\n","        ((0, 0), 'D'): {(1, 0): 1.0},\n","        ((0, 0), 'L'): {(0, 0): 1.0},\n","        ((0, 0), 'R'): {(0, 1): 1.0},\n","        ((0, 1), 'U'): {(0, 1): 1.0},\n","        ((0, 1), 'D'): {(0, 1): 1.0},\n","        ((0, 1), 'L'): {(0, 0): 1.0},\n","        ((0, 1), 'R'): {(0, 2): 1.0},\n","        ((0, 2), 'U'): {(0, 2): 1.0},\n","        ((0, 2), 'D'): {(1, 2): 1.0},\n","        ((0, 2), 'L'): {(0, 1): 1.0},\n","        ((0, 2), 'R'): {(0, 3): 1.0},\n","        ((2, 1), 'U'): {(2, 1): 1.0},\n","        ((2, 1), 'D'): {(2, 1): 1.0},\n","        ((2, 1), 'L'): {(2, 0): 1.0},\n","        ((2, 1), 'R'): {(2, 2): 1.0},\n","        ((2, 2), 'U'): {(1, 2): 1.0},\n","        ((2, 2), 'D'): {(2, 2): 1.0},\n","        ((2, 2), 'L'): {(2, 1): 1.0},\n","        ((2, 2), 'R'): {(2, 3): 1.0},\n","        ((2, 3), 'U'): {(1, 3): 1.0},\n","        ((2, 3), 'D'): {(2, 3): 1.0},\n","        ((2, 3), 'L'): {(2, 2): 1.0},\n","        ((2, 3), 'R'): {(2, 3): 1.0},\n","        ((1, 2), 'U'): {(0, 2): 0.5, (1, 3): 0.5},\n","        ((1, 2), 'D'): {(2, 2): 1.0},\n","        ((1, 2), 'L'): {(1, 2): 1.0},\n","        ((1, 2), 'R'): {(1, 3): 1.0},\n","    }\n","\n","    g.set_rewards_actions_probs(rewards, actions, probs)\n","\n","    return g"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PdcpSB_xZK0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598005023295,"user_tz":-420,"elapsed":4599,"user":{"displayName":"Hendra Ronaldi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1GApVeWOLfLtzaKM_vycD2e04Atp2_i5j6_Ol=s64","userId":"08624543525853224756"}}},"source":["def grid_windy_penalized(step_cost = -0.1):\n","    g = WindyGrid(3, 4, (2, 0))\n","    rewards = {\n","        (0, 0): step_cost,\n","        (0, 1): step_cost,\n","        (0, 2): step_cost,\n","        (1, 0): step_cost,\n","        (1, 2): step_cost,\n","        (2, 0): step_cost,\n","        (2, 1): step_cost,\n","        (2, 2): step_cost,\n","        (2, 3): step_cost,\n","        (0, 3): 1,\n","        (1, 3): -1\n","    }\n","    \n","    actions = {\n","        (0, 0): ('D', 'R'),\n","        (0, 1): ('L', 'R'),\n","        (0, 2): ('L', 'D', 'R'),\n","        (1, 0): ('U', 'D'),\n","        (1, 2): ('U', 'D', 'R'),\n","        (2, 0): ('U', 'R'),\n","        (2, 1): ('L', 'R'),\n","        (2, 2): ('U', 'L', 'R'),\n","        (2, 3): ('U', 'L')\n","    }\n","\n","    probs = {\n","        ((2, 0), 'U'): {(1, 0): 1.0},\n","        ((2, 0), 'D'): {(2, 0): 1.0},\n","        ((2, 0), 'L'): {(2, 0): 1.0},\n","        ((2, 0), 'R'): {(2, 1): 1.0},\n","        ((1, 0), 'U'): {(0, 0): 1.0},\n","        ((1, 0), 'D'): {(2, 0): 1.0},\n","        ((1, 0), 'L'): {(1, 0): 1.0},\n","        ((1, 0), 'R'): {(1, 0): 1.0},\n","        ((0, 0), 'U'): {(0, 0): 1.0},\n","        ((0, 0), 'D'): {(1, 0): 1.0},\n","        ((0, 0), 'L'): {(0, 0): 1.0},\n","        ((0, 0), 'R'): {(0, 1): 1.0},\n","        ((0, 1), 'U'): {(0, 1): 1.0},\n","        ((0, 1), 'D'): {(0, 1): 1.0},\n","        ((0, 1), 'L'): {(0, 0): 1.0},\n","        ((0, 1), 'R'): {(0, 2): 1.0},\n","        ((0, 2), 'U'): {(0, 2): 1.0},\n","        ((0, 2), 'D'): {(1, 2): 1.0},\n","        ((0, 2), 'L'): {(0, 1): 1.0},\n","        ((0, 2), 'R'): {(0, 3): 1.0},\n","        ((2, 1), 'U'): {(2, 1): 1.0},\n","        ((2, 1), 'D'): {(2, 1): 1.0},\n","        ((2, 1), 'L'): {(2, 0): 1.0},\n","        ((2, 1), 'R'): {(2, 2): 1.0},\n","        ((2, 2), 'U'): {(1, 2): 1.0},\n","        ((2, 2), 'D'): {(2, 2): 1.0},\n","        ((2, 2), 'L'): {(2, 1): 1.0},\n","        ((2, 2), 'R'): {(2, 3): 1.0},\n","        ((2, 3), 'U'): {(1, 3): 1.0},\n","        ((2, 3), 'D'): {(2, 3): 1.0},\n","        ((2, 3), 'L'): {(2, 2): 1.0},\n","        ((2, 3), 'R'): {(2, 3): 1.0},\n","        ((1, 2), 'U'): {(0, 2): 0.5, (1, 3): 0.5},\n","        ((1, 2), 'D'): {(2, 2): 1.0},\n","        ((1, 2), 'L'): {(1, 2): 1.0},\n","        ((1, 2), 'R'): {(1, 3): 1.0},\n","    }\n","\n","    g.set_rewards_actions_probs(rewards, actions, probs)\n","\n","    return g"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"id":"3drjW76Letjh","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598005023296,"user_tz":-420,"elapsed":4595,"user":{"displayName":"Hendra Ronaldi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1GApVeWOLfLtzaKM_vycD2e04Atp2_i5j6_Ol=s64","userId":"08624543525853224756"}}},"source":["def print_values(V, g):\n","    for i in range(g.rows):\n","        print(\"-------------------------\")\n","        for j in range(g.columns):\n","            v = V.get((i, j), 0)\n","            if v >= 0:\n","                print(\" %.2f|\" % v, end=\"\")\n","            else:\n","                print(\"%.2f|\" % v, end=\"\")\n","        print(\"\")\n","\n","def print_policy(P, g):\n","    for i in range(g.rows):\n","        print(\"-------------------------\")\n","        for j in range(g.columns):\n","            a = P.get((i, j), ' ')\n","            print(\" %s |\" % a, end=\"\")\n","        print(\"\")"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"id":"EptCSbcFNNWT","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598005023297,"user_tz":-420,"elapsed":4586,"user":{"displayName":"Hendra Ronaldi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1GApVeWOLfLtzaKM_vycD2e04Atp2_i5j6_Ol=s64","userId":"08624543525853224756"}}},"source":["def t_probs_rewards(g):\n","    transition_probs = {}\n","\n","    rewards = {}\n","\n","    for i in range(g.rows):\n","        for j in range(g.columns):\n","            s = (i, j)\n","            if not g.is_terminal(s):\n","                for a in ACTION_SPACE:\n","                    next_states = g.probs[(s, a)]\n","                    for s_next in next_states.keys():\n","                        transition_probs[(s, a, s_next)] = next_states[s_next] # prob of s with action to reach s_next is probabilistic\n","                        rewards[(s, a, s_next)] = g.rewards.get(s_next, 0)\n","    return transition_probs, rewards"],"execution_count":70,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ytkacPGN7Ilq","colab_type":"text"},"source":["## Policy Iteration using policy evaluation and optimization"]},{"cell_type":"code","metadata":{"id":"XvHbZzYcQ2he","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598005023298,"user_tz":-420,"elapsed":4583,"user":{"displayName":"Hendra Ronaldi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1GApVeWOLfLtzaKM_vycD2e04Atp2_i5j6_Ol=s64","userId":"08624543525853224756"}}},"source":["def evaluate_deterministic_policy(g, policy):\n","    V = {}\n","    for s in g.all_states():\n","        V[s] = 0\n","    iterations = 0\n","\n","    while True:\n","        biggest_change = 0\n","        for s in g.all_states():\n","            old_v = V[s]\n","            new_v = 0\n","            for a in ACTION_SPACE:\n","                for s_next in g.all_states():\n","                    action_prob = 1 if policy.get(s) == a else 0\n","\n","                    r = rewards.get((s, a, s_next), 0)\n","\n","                    new_v += action_prob * transition_probs.get((s, a, s_next), 0) * (r + gamma * V[s_next])\n","\n","            V[s] = new_v\n","            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n","\n","        iterations += 1\n","        if biggest_change < delta:\n","            break\n","\n","    return V"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSlIBONPxOMq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598005023299,"user_tz":-420,"elapsed":4579,"user":{"displayName":"Hendra Ronaldi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1GApVeWOLfLtzaKM_vycD2e04Atp2_i5j6_Ol=s64","userId":"08624543525853224756"}}},"source":["def get_optimal_policy(g, policy):\n","    while True:\n","        V = evaluate_deterministic_policy(g, policy)\n","\n","        is_policy_converged = True\n","        for s in g.actions.keys():\n","            old_a = policy[s]\n","            new_a = None\n","            best_value = float('-inf')\n","\n","            for a in ACTION_SPACE:\n","                v = 0\n","                for s_next in g.all_states():\n","                    r = rewards.get((s, a, s_next), 0)\n","                    v += transition_probs.get((s, a, s_next), 0) * (r + gamma * V[s_next])\n","\n","                if v > best_value:\n","                    best_value = v\n","                    new_a = a\n","            \n","            policy[s] = new_a\n","\n","            if new_a != old_a:\n","                is_policy_converged = False\n","            \n","        if is_policy_converged:\n","            break\n","    return V, policy"],"execution_count":72,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"exx3xBcv6_W7","colab_type":"text"},"source":["## Alternative Function: Value Iteration"]},{"cell_type":"code","metadata":{"id":"1GdmxxV_5ACZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598005023300,"user_tz":-420,"elapsed":4575,"user":{"displayName":"Hendra Ronaldi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1GApVeWOLfLtzaKM_vycD2e04Atp2_i5j6_Ol=s64","userId":"08624543525853224756"}}},"source":["def value_iteration(g):\n","    V = {}\n","    for s in g.all_states():\n","        V[s] = 0\n","    iterations = 0\n","    while True:\n","        biggest_change = 0\n","        for s in g.all_states():\n","            if not g.is_terminal(s):\n","                old_v = V[s]\n","                new_v = float('-inf')\n","                for a in ACTION_SPACE:\n","                    v = 0\n","                    for s_next in g.all_states():\n","                        r = rewards.get((s, a, s_next), 0)\n","\n","                        v += transition_probs.get((s, a, s_next), 0) * (r + gamma * V[s_next])\n","\n","                    if v > new_v:\n","                        new_v = v\n","                V[s] = new_v\n","                biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n","\n","        iterations += 1\n","        if biggest_change < delta:\n","            break\n","    return V\n","\n","def get_optimal_policy_to_optimal_value(g, V):\n","    policy = {}\n","    for s in g.actions.keys():\n","        best_a = None\n","        best_value = float('-inf')\n","\n","        for a in ACTION_SPACE:\n","            v = 0\n","            for s_next in g.all_states():\n","                r = rewards.get((s, a, s_next), 0)\n","                v += transition_probs.get((s, a, s_next), 0) * (r + gamma * V[s_next])\n","\n","            if v > best_value:\n","                best_value = v\n","                best_a = a\n","        \n","        policy[s] = best_a\n","    return policy"],"execution_count":73,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6deyZkgz62ff","colab_type":"text"},"source":["## Policy Iteration"]},{"cell_type":"code","metadata":{"id":"kYIDl_tUi8P6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1598005023301,"user_tz":-420,"elapsed":4568,"user":{"displayName":"Hendra Ronaldi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1GApVeWOLfLtzaKM_vycD2e04Atp2_i5j6_Ol=s64","userId":"08624543525853224756"}},"outputId":"15c097a5-26fb-42aa-ddff-6f94d4f5a42e"},"source":["g = grid_windy()\n","delta = 1e-3\n","gamma = 0.9\n","\n","# random policy\n","policy = {}\n","for s in g.actions.keys():\n","    policy[s] = np.random.choice(ACTION_SPACE)\n","\n","print(\"initial policy\")\n","print_policy(policy, g)\n","print(\"\")\n","\n","transition_probs, rewards = t_probs_rewards(g)\n","V, policy = get_optimal_policy(g, policy)\n","print(\"optimal policy\")\n","print_policy(policy, g)\n","print(\"V\")\n","print_values(V, g)"],"execution_count":74,"outputs":[{"output_type":"stream","text":["initial policy\n","-------------------------\n"," U | U | D |   |\n","-------------------------\n"," D |   | R |   |\n","-------------------------\n"," D | D | D | D |\n","\n","optimal policy\n","-------------------------\n"," R | R | R |   |\n","-------------------------\n"," U |   | D |   |\n","-------------------------\n"," U | L | L | L |\n","V\n","-------------------------\n"," 0.81| 0.90| 1.00| 0.00|\n","-------------------------\n"," 0.73| 0.00| 0.48| 0.00|\n","-------------------------\n"," 0.66| 0.59| 0.53| 0.48|\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rJPXvrL2padB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1598005023302,"user_tz":-420,"elapsed":4561,"user":{"displayName":"Hendra Ronaldi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1GApVeWOLfLtzaKM_vycD2e04Atp2_i5j6_Ol=s64","userId":"08624543525853224756"}},"outputId":"ef6d8546-6f09-4e21-ef4a-b539628b800d"},"source":["g = grid_windy_penalized(-0.15)\n","delta = 1e-3\n","gamma = 0.9\n","\n","# random policy\n","policy = {}\n","for s in g.actions.keys():\n","    policy[s] = np.random.choice(ACTION_SPACE)\n","\n","print(\"initial policy\")\n","print_policy(policy, g)\n","print(\"\")\n","\n","transition_probs, rewards = t_probs_rewards(g)\n","V, policy = get_optimal_policy(g, policy)\n","print(\"optimal policy\")\n","print_policy(policy, g)\n","print(\"V\")\n","print_values(V, g)"],"execution_count":75,"outputs":[{"output_type":"stream","text":["initial policy\n","-------------------------\n"," R | D | R |   |\n","-------------------------\n"," U |   | R |   |\n","-------------------------\n"," D | R | L | L |\n","\n","optimal policy\n","-------------------------\n"," R | R | R |   |\n","-------------------------\n"," U |   | U |   |\n","-------------------------\n"," U | L | L | L |\n","V\n","-------------------------\n"," 0.53| 0.75| 1.00| 0.00|\n","-------------------------\n"," 0.32| 0.00|-0.12| 0.00|\n","-------------------------\n"," 0.14|-0.02|-0.17|-0.30|\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7IBUdCS4662x","colab_type":"text"},"source":["## Value Iteration"]},{"cell_type":"code","metadata":{"id":"horpSG5503Y3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"ok","timestamp":1598005023303,"user_tz":-420,"elapsed":4554,"user":{"displayName":"Hendra Ronaldi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1GApVeWOLfLtzaKM_vycD2e04Atp2_i5j6_Ol=s64","userId":"08624543525853224756"}},"outputId":"cafe3feb-45ed-48e6-e4a2-169a7bef04f0"},"source":["g = grid_windy()\n","delta = 1e-3\n","gamma = 0.9\n","\n","# random policy\n","policy = {}\n","for s in g.actions.keys():\n","    policy[s] = np.random.choice(ACTION_SPACE)\n","\n","print(\"initial policy\")\n","print_policy(policy, g)\n","print(\"\")\n","\n","transition_probs, rewards = t_probs_rewards(g)\n","V = value_iteration(g)\n","print(\"optimal V\")\n","print_values(V, g)\n","\n","policy = get_optimal_policy_to_optimal_value(g, V)\n","print(\"optimal policy\")\n","print_policy(policy, g)"],"execution_count":76,"outputs":[{"output_type":"stream","text":["initial policy\n","-------------------------\n"," R | D | U |   |\n","-------------------------\n"," R |   | L |   |\n","-------------------------\n"," R | R | U | L |\n","\n","optimal V\n","-------------------------\n"," 0.81| 0.90| 1.00| 0.00|\n","-------------------------\n"," 0.73| 0.00| 0.48| 0.00|\n","-------------------------\n"," 0.66| 0.59| 0.53| 0.48|\n","optimal policy\n","-------------------------\n"," R | R | R |   |\n","-------------------------\n"," U |   | D |   |\n","-------------------------\n"," U | L | L | L |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZUDiQvyK6rMB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"ok","timestamp":1598005054969,"user_tz":-420,"elapsed":1478,"user":{"displayName":"Hendra Ronaldi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1GApVeWOLfLtzaKM_vycD2e04Atp2_i5j6_Ol=s64","userId":"08624543525853224756"}},"outputId":"390dec1e-c4a0-4e2e-a67c-4f01c9d998c2"},"source":["g = grid_windy_penalized(-0.1)\n","delta = 1e-3\n","gamma = 0.9\n","\n","# random policy\n","policy = {}\n","for s in g.actions.keys():\n","    policy[s] = np.random.choice(ACTION_SPACE)\n","\n","print(\"initial policy\")\n","print_policy(policy, g)\n","print(\"\")\n","\n","transition_probs, rewards = t_probs_rewards(g)\n","V = value_iteration(g)\n","print(\"optimal V\")\n","print_values(V, g)\n","\n","policy = get_optimal_policy_to_optimal_value(g, V)\n","print(\"optimal policy\")\n","print_policy(policy, g)"],"execution_count":80,"outputs":[{"output_type":"stream","text":["initial policy\n","-------------------------\n"," L | L | R |   |\n","-------------------------\n"," L |   | U |   |\n","-------------------------\n"," U | R | U | R |\n","\n","optimal V\n","-------------------------\n"," 0.62| 0.80| 1.00| 0.00|\n","-------------------------\n"," 0.46| 0.00|-0.04| 0.00|\n","-------------------------\n"," 0.31| 0.18| 0.06|-0.04|\n","optimal policy\n","-------------------------\n"," R | R | R |   |\n","-------------------------\n"," U |   | D |   |\n","-------------------------\n"," U | L | L | L |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J1oIpO_ehb9s","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}